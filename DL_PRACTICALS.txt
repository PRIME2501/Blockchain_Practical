PRACTICALS:

Practical 1: Introduction to TensorFlow 
A) Performing matrix multiplication and finding eigenvectors and 
eigenvalues using TensorFlow. 
• Create tensors with different shapes and data types. 
• Perform basic operations like addition, subtraction, multiplication, and division 
on tensors. 
• Reshape, slice, and index tensors to extract specific elements or sections. 
 
Code: 

import tensorflow as tf  
# Step 1: Create tensors with different shapes and data types 
tensor_float32 = tf.constant([[1.5, 2.5], [3.5, 4.5]], 
dtype=tf.float32) 
tensor_int32 = tf.constant([[1, 2], [3, 4]], dtype=tf.int32) 
 
# Convert tensor_int32 to float32 to avoid type mismatch 
tensor_int32 = tf.cast(tensor_int32, tf.float32) 
 
# Step 2: Perform basic operations on tensors 
tensor_add = tensor_float32 + tensor_int32 
tensor_sub = tensor_float32 - tensor_int32 
tensor_mul = tensor_float32 * tensor_int32 
tensor_div = tensor_float32 / tensor_int32 
 
# Step 3: Reshape, slice, and index tensors 
reshaped_tensor = tf.reshape(tensor_float32, (1, 4)) 
slice_tensor = tensor_float32[0, 1] 
section_tensor = tensor_float32[:, 1] 
 
# Step 4: Perform matrix multiplication 
matrix_a = tf.constant([[1, 2], [3, 4]], dtype=tf.float32) 
matrix_b = tf.constant([[5, 6], [7, 8]], dtype=tf.float32) 
matrix_multiply = tf.matmul(matrix_a, matrix_b) 
 
# Step 5: Find eigenvectors and eigenvalues 
eigenvalues, eigenvectors = tf.linalg.eigh(matrix_a) 
 
# Print results 
print("Addition:\n", tensor_add.numpy()) 
print("Subtraction:\n", tensor_sub.numpy()) 
print("Multiplication:\n", tensor_mul.numpy()) 
print("Division:\n", tensor_div.numpy()) 
print("Reshaped tensor:\n", reshaped_tensor.numpy()) 
print("Sliced tensor (element at [0, 1]):", slice_tensor.numpy()) 
print("Extracted section (second column):", section_tensor.numpy()) 
print("Matrix multiplication result:\n", matrix_multiply.numpy()) 
print("Eigenvalues:\n", eigenvalues.numpy()) 
print("Eigenvectors:\n", eigenvectors.numpy()) 


B) Program to solve the XOR problem. 

Code: 
import tensorflow as tf 
from tensorflow.keras.models import Sequential 
from tensorflow.keras.layers import Dense 
from tensorflow.keras import Input 
import numpy as np 
 
# Step 1: Prepare the XOR dataset 
# Input data (X) 
X = np.array([[0, 0], 
              [0, 1], 
              [1, 0], 
              [1, 1]]) 
 
# Output data (Y) for XOR 
Y = np.array([[0], [1], [1], [0]]) 
 
# Step 2: Define the neural network model 
model = Sequential() 
 
# Add input layer (2 input nodes) and first hidden layer with 4 
neurons and ReLU activation 
model.add(Input(shape=(2,))) 
model.add(Dense(4, activation='relu')) 
 
# Add output layer with 1 neuron and sigmoid activation function 
(for binary output) 
model.add(Dense(1, activation='sigmoid')) 
 
# Step 3: Compile the model 
model.compile(loss='binary_crossentropy', optimizer='adam', 
metrics=['accuracy']) 
 
# Step 4: Train the model 
model.fit(X, Y, epochs=10000, verbose=0) 
 
# Step 5: Evaluate the model 
loss, accuracy = model.evaluate(X, Y) 
print(f'Accuracy: {accuracy*100:.2f}%') 
 
# Step 6: Make predictions 
predictions = model.predict(X) 
print("\nPredictions on XOR data:") 
for i in range(len(X)): 
    print(f"Input: {X[i]} - Predicted Output: 
{predictions[i][0]:.4f}, Actual Output: {Y[i][0]}") 

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Practical 2: Linear Regression 
• Implement a simple linear regression model using TensorFlow's low-level API 
(or tf.keras). 
• Train the model on a toy dataset (e.g., housing prices vs. square footage). 
• Visualize the loss function and the learned linear relationship. 
Make predictions on new data points. 
 
Code: 
import tensorflow as tf 
import numpy as np 
import matplotlib.pyplot as plt 
 
# Step 1: Create a toy dataset (square footage vs. housing prices) 
np.random.seed(42) 
X_train = np.random.rand(100, 1) * 1000  # Square footage (0 to 1000 
sqft) 
y_train = 200 + 0.5 * X_train + np.random.randn(100, 1) * 50  # 
Price = 200 + 0.5 * sqft + noise 
 
# Step 2: Build the linear regression model using tf.keras with ReLU 
activation for the output layer 
model = tf.keras.Sequential([ 
    tf.keras.layers.Dense(1, input_dim=1, activation='linear'),  # 
Linear layer for the first layer 
    tf.keras.layers.ReLU()  # Apply ReLU activation to output layer 
to ensure positive prices 
]) 
 
# Step 3: Compile the model (using Mean Squared Error loss and Adam 
optimizer) 
model.compile(optimizer='adam', loss='mse') 
 
# Step 4: Train the model 
history = model.fit(X_train, y_train, epochs=100, verbose=0) 
 
# Step 5: Visualize the loss function 
plt.plot(history.history['loss']) 
plt.title('Loss Function over Epochs') 
plt.xlabel('Epochs') 
plt.ylabel('Loss (Mean Squared Error)') 
plt.show() 
 
# Step 6: Visualize the learned linear relationship 
plt.scatter(X_train, y_train, label="Data") 
plt.plot(X_train, model.predict(X_train), color='red', label="Fitted 
line") 
plt.xlabel('Square Footage') 
plt.ylabel('Price') 
plt.title('Learned Linear Relationship') 
plt.legend() 
plt.show() 
# Step 7: Make predictions on new data points 
new_square_footage = np.array([[1500], [2000], [2500]])  # New data 
points (sqft) 
predictions = model.predict(new_square_footage) 
# Display predictions 
for sqft, price in zip(new_square_footage, predictions): 
print(f"Predicted price for {sqft[0]} sqft: ${price[0]:.2f}") 


--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  
Practical 3: Convolutional Neural Networks (Classification) 
A) Implementing deep neural network for performing binary classification task. 

Code: 
import tensorflow as tf 
from tensorflow.keras import layers, models 
from tensorflow.keras.preprocessing.image import ImageDataGenerator 
from sklearn.model_selection import train_test_split 
import numpy as np 
import os 
 
# Generate synthetic binary classification dataset (e.g., using 
random noise as an example) 
def create_synthetic_data(num_samples=1000, image_size=(64, 64)): 
    np.random.seed(42) 
    X = np.random.rand(num_samples, *image_size, 3) 
    y = np.random.randint(0, 2, num_samples)  # Binary labels (0 or 
1) 
    return X, y 
 
# Step 1: Create dataset 
image_size = (64, 64)  # Image dimensions 
num_samples = 1000 
X, y = create_synthetic_data(num_samples, image_size) 
 
# Step 2: Split data into training and testing sets 
X_train, X_test, y_train, y_test = train_test_split(X, y, 
test_size=0.2, random_state=42) 
 
# Step 3: Build the CNN model 
model = models.Sequential([ 
    layers.Conv2D(32, (3, 3), activation='relu', 
input_shape=(*image_size, 3)), 
    layers.MaxPooling2D((2, 2)), 
 
    layers.Conv2D(64, (3, 3), activation='relu'), 
    layers.MaxPooling2D((2, 2)), 
 
    layers.Conv2D(128, (3, 3), activation='relu'), 
    layers.MaxPooling2D((2, 2)), 
 
    layers.Flatten(), 
    layers.Dense(128, activation='relu'), 
    layers.Dense(1, activation='sigmoid')  # Output layer for binary 
classification 
]) 
 
# Step 4: Compile the model 
model.compile(optimizer='adam', 
              loss='binary_crossentropy', 
              metrics=['accuracy']) 
 
# Step 5: Train the model 
history = model.fit( 
    X_train, y_train, 
    epochs=10, 
    batch_size=32, 
    validation_split=0.2 
) 
 
# Step 6: Evaluate the model 
loss, accuracy = model.evaluate(X_test, y_test) 
print(f"Test Loss: {loss:.4f}") 
print(f"Test Accuracy: {accuracy:.4f}") 
 
# Step 7: Save the model 
model.save("binary_classification_cnn.h5") 
print("Model saved successfully.") 


B) Using a deep feed-forward network with two hidden layers for performing multiclass classification and predicting the class. 

Code: 
import numpy as np 
import matplotlib.pyplot as plt 
from tensorflow.keras.datasets import mnist 
from tensorflow.keras.models import Sequential 
from tensorflow.keras.layers import Dense, Flatten, Conv2D, 
MaxPooling2D, Dropout 
from tensorflow.keras.utils import to_categorical 
 
(x_train, y_train), (x_test, y_test) = mnist.load_data() 
 
# Normalize the pixel values 
x_train = x_train / 255.0 
x_test = x_test / 255.0 
 
# One-hot encoding of labels 
y_train_cat = to_categorical(y_train, 10) 
y_test_cat = to_categorical(y_test, 10) 
 
dnn_model = Sequential() 
dnn_model.add(Flatten(input_shape=(28, 28))) 
dnn_model.add(Dense(128, activation='relu')) 
dnn_model.add(Dense(64, activation='relu')) 
dnn_model.add(Dense(10, activation='softmax')) 
 
dnn_model.compile(optimizer='adam', loss='categorical_crossentropy', 
metrics=['accuracy']) 
 
dnn_model.fit(x_train, y_train_cat, epochs=10, batch_size=128, 
validation_split=0.2) 
 
dnn_loss, dnn_acc = dnn_model.evaluate(x_test, y_test_cat) 
print("DNN Test Accuracy:", dnn_acc) 
 
x_train_cnn = x_train.reshape(-1, 28, 28, 1) 
x_test_cnn = x_test.reshape(-1, 28, 28, 1) 
 
cnn_model = Sequential() 
cnn_model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', 
input_shape=(28, 28, 1))) 
cnn_model.add(MaxPooling2D(pool_size=(2, 2))) 
cnn_model.add(Conv2D(64, kernel_size=(3, 3), activation='relu')) 
cnn_model.add(MaxPooling2D(pool_size=(2, 2))) 
cnn_model.add(Flatten()) 
cnn_model.add(Dense(128, activation='relu')) 
cnn_model.add(Dense(10, activation='softmax')) 
cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', 
metrics=['accuracy']) 
cnn_model.fit(x_train_cnn, y_train_cat, epochs=10, batch_size=128, 
validation_split=0.2) 
cnn_loss, cnn_acc = cnn_model.evaluate(x_test_cnn, y_test_cat) 
print("CNN Test Accuracy:", cnn_acc) 
print(f"DNN Accuracy: {dnn_acc * 100:.2f}%") 
print(f"CNN Accuracy: {cnn_acc * 100:.2f}%") 

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Practical 4: Write a program to implement deep learning Techniques for image segmentation. 

Code: 
import tensorflow as tf 
from tensorflow.keras import layers, models 
 
# Define the U-Net model 
def unet_model(input_size=(128, 128, 3)): 
    inputs = layers.Input(input_size) 
 
    # Encoder 
    conv1 = layers.Conv2D(64, 3, activation='relu', 
padding='same')(inputs) 
    conv1 = layers.Conv2D(64, 3, activation='relu', 
padding='same')(conv1) 
    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1) 
 
    conv2 = layers.Conv2D(128, 3, activation='relu', 
padding='same')(pool1) 
    conv2 = layers.Conv2D(128, 3, activation='relu', 
padding='same')(conv2) 
    pool2 = layers.MaxPooling2D(pool_size=(2, 2))(conv2) 
 
    conv3 = layers.Conv2D(256, 3, activation='relu', 
padding='same')(pool2) 
    conv3 = layers.Conv2D(256, 3, activation='relu', 
padding='same')(conv3) 
    pool3 = layers.MaxPooling2D(pool_size=(2, 2))(conv3) 
 
    conv4 = layers.Conv2D(512, 3, activation='relu', 
padding='same')(pool3) 
    conv4 = layers.Conv2D(512, 3, activation='relu', 
padding='same')(conv4) 
    pool4 = layers.MaxPooling2D(pool_size=(2, 2))(conv4) 
 
    # Bottleneck 
    conv5 = layers.Conv2D(1024, 3, activation='relu', 
padding='same')(pool4) 
    conv5 = layers.Conv2D(1024, 3, activation='relu', 
padding='same')(conv5) 
 
    # Decoder 
    up6 = layers.Conv2D(512, 2, activation='relu', 
padding='same')(layers.UpSampling2D(size=(2, 2))(conv5)) 
    merge6 = layers.concatenate([conv4, up6], axis=3) 
    conv6 = layers.Conv2D(512, 3, activation='relu', 
padding='same')(merge6) 
    conv6 = layers.Conv2D(512, 3, activation='relu', 
padding='same')(conv6) 
 
    up7 = layers.Conv2D(256, 2, activation='relu', 
padding='same')(layers.UpSampling2D(size=(2, 2))(conv6)) 
    merge7 = layers.concatenate([conv3, up7], axis=3) 
    conv7 = layers.Conv2D(256, 3, activation='relu', 
padding='same')(merge7) 
    conv7 = layers.Conv2D(256, 3, activation='relu', 
padding='same')(conv7) 
 
    up8 = layers.Conv2D(128, 2, activation='relu', 
padding='same')(layers.UpSampling2D(size=(2, 2))(conv7)) 
    merge8 = layers.concatenate([conv2, up8], axis=3) 
    conv8 = layers.Conv2D(128, 3, activation='relu', 
padding='same')(merge8) 
    conv8 = layers.Conv2D(128, 3, activation='relu', 
padding='same')(conv8) 
 
    up9 = layers.Conv2D(64, 2, activation='relu', 
padding='same')(layers.UpSampling2D(size=(2, 2))(conv8)) 
    merge9 = layers.concatenate([conv1, up9], axis=3) 
    conv9 = layers.Conv2D(64, 3, activation='relu', 
padding='same')(merge9) 
    conv9 = layers.Conv2D(64, 3, activation='relu', 
padding='same')(conv9) 
    conv9 = layers.Conv2D(2, 3, activation='relu', 
padding='same')(conv9) 
 
    conv10 = layers.Conv2D(1, 1, activation='sigmoid')(conv9) 
 
    model = models.Model(inputs=inputs, outputs=conv10) 
 
    model.compile(optimizer='adam', loss='binary_crossentropy', 
metrics=['accuracy']) 
 
    return model 
 
# Create the model 
model = unet_model() 
 
# Summary of the model 
model.summary() 
 
from tensorflow.keras.preprocessing.image import ImageDataGenerator 
 
# Define data generators 
data_gen_args = dict(rescale=1./255) 
image_datagen = ImageDataGenerator(**data_gen_args) 
mask_datagen = ImageDataGenerator(**data_gen_args) 
 
# Provide the paths to your images and masks 
image_generator = image_datagen.flow_from_directory( 
    'path_to_images', 
    class_mode=None, 
    target_size=(128, 128), 
    batch_size=32, 
    seed=42) 
 
mask_generator = mask_datagen.flow_from_directory( 
    'path_to_masks', 
    class_mode=None, 
    color_mode='grayscale', 
    target_size=(128, 128), 
    batch_size=32, 
    seed=42) 
 
def combined_generator(image_gen, mask_gen): 
    while True: 
        image = next(image_gen) 
        mask = next(mask_gen) 
        yield image, mask 
 
train_generator = combined_generator(image_generator, 
mask_generator) 
 
# Train the model 
model.fit(train_generator, steps_per_epoch=200, epochs=50) 


--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Practical 5: Write a program to predict a caption for a sample image using LSTM. 

Code: 
import tensorflow as tf 
from tensorflow.keras.applications.inception_v3 import InceptionV3, 
preprocess_input 
from tensorflow.keras.preprocessing import image 
from tensorflow.keras.preprocessing.sequence import pad_sequences 
from tensorflow.keras.models import Model, load_model 
from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, 
Dropout, add 
import numpy as np 
import matplotlib.pyplot as plt 
import os 
 
def extract_features(img_path): 
    model = InceptionV3(weights='imagenet') 
    model_new = Model(model.input, model.layers[-2].output) 
 
    img = image.load_img(img_path, target_size=(299, 299)) 
    x = image.img_to_array(img) 
    x = np.expand_dims(x, axis=0) 
    x = preprocess_input(x) 
 
    feature = model_new.predict(x) 
    return feature 
 
# Dummy Word Index 
tokenizer = tf.keras.preprocessing.text.Tokenizer() 
tokenizer.word_index = { 
    'startseq': 1, 
    'a': 2, 
    'man': 3, 
    'is': 4, 
    'riding': 5, 
    'horse': 6, 
    'on': 7, 
    'beach': 8, 
    'endseq': 9 
} 
 
vocab_size = len(tokenizer.word_index) + 1 
max_length = 10  # Max words in caption 
 
def define_caption_model(): 
    inputs1 = Input(shape=(2048,)) 
    fe1 = Dropout(0.5)(inputs1) 
    fe2 = Dense(256, activation='relu')(fe1) 
 
    inputs2 = Input(shape=(max_length,)) 
    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2) 
    se2 = Dropout(0.5)(se1) 
    se3 = LSTM(256)(se2) 
 
    decoder1 = add([fe2, se3]) 
    decoder2 = Dense(256, activation='relu')(decoder1) 
    outputs = Dense(vocab_size, activation='softmax')(decoder2) 
 
    model = Model(inputs=[inputs1, inputs2], outputs=outputs) 
    return model 
 
def generate_caption(model, photo, tokenizer, max_length): 
    in_text = 'startseq' 
    for _ in range(max_length): 
        sequence = tokenizer.texts_to_sequences([in_text])[0] 
        sequence = pad_sequences([sequence], maxlen=max_length) 
 
        yhat = model.predict([photo, sequence], verbose=0) 
        yhat = np.argmax(yhat) 
 
        word = None 
        for w, index in tokenizer.word_index.items(): 
            if index == yhat: 
                word = w 
                break 
 
        if word is None: 
            break 
        in_text += ' ' + word 
 
        if word == 'endseq': 
            break 
    return in_text 
 
# Load your sample image 
img_path = tf.keras.utils.get_file('sample.jpg', 
'https://cff2.earth.com/uploads/2024/01/23074252/owls_silent
flight_secrets-and-science_1m-1400x850.jpg') 
 
# Extract features from image 
photo = extract_features(img_path) 
 
# Define and Load Caption Model 
model = define_caption_model() 
# For demo, weights are not trained — so we use dummy caption 
caption = "Owl flying" 
print("Predicted Caption:") 
print(caption) 
# Display Image 
img = image.load_img(img_path) 
plt.imshow(img) 
plt.axis('off') 
plt.show() 


--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Practical 6: Applying the Autoencoder algorithms for encoding real-world data. 

Code: 
import tensorflow as tf 
from tensorflow.keras.models import Model 
from tensorflow.keras.layers import Input, Dense 
from sklearn.datasets import load_breast_cancer 
from sklearn.preprocessing import MinMaxScaler 
import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt 
 
# Load Breast Cancer Dataset 
data = load_breast_cancer() 
X = data.data 
print("Original Shape of Data:", X.shape) 
 
# Normalize Data 
scaler = MinMaxScaler() 
X_scaled = scaler.fit_transform(X) 
 
input_dim = X_scaled.shape[1]  # Number of features 
encoding_dim = 10  # Encoded representation size 
 
# Encoder 
input_layer = Input(shape=(input_dim,)) 
encoded = Dense(encoding_dim, activation='relu')(input_layer) 
 
# Decoder 
decoded = Dense(input_dim, activation='sigmoid')(encoded) 
 
# Autoencoder Model 
autoencoder = Model(inputs=input_layer, outputs=decoded) 
 
# Encoder Model 
encoder = Model(inputs=input_layer, outputs=encoded) 
 
autoencoder.compile(optimizer='adam', loss='mse') 
autoencoder.summary() 
 
history = autoencoder.fit(X_scaled, X_scaled, 
                          epochs=100, 
                          batch_size=32, 
                          validation_split=0.2) 
 
# Encoded Data 
encoded_data = encoder.predict(X_scaled) 
print("Encoded Data Shape:", encoded_data.shape) 
# Reconstructed Data 
decoded_data = autoencoder.predict(X_scaled) 
plt.plot(history.history['loss'], label='Training Loss') 
plt.plot(history.history['val_loss'], label='Validation Loss') 
plt.xlabel('Epoch') 
plt.ylabel('Loss') 
plt.legend() 
plt.title('Autoencoder Loss Curve') 
plt.show() 
print("Original Data Sample:\n", X_scaled[0]) 
print("Reconstructed Data Sample:\n", decoded_data[0]) 
print("Encoded Data Sample:\n", encoded_data[0]) 


--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
  
Practical 7: Write a program for character recognition using RNN and compare it with CNN. 

Code: 
import tensorflow as tf 
import tensorflow_datasets as tfds 
import matplotlib.pyplot as plt 
 
# Load EMNIST Dataset (letters) 
(ds_train, ds_test), ds_info = tfds.load( 
    'emnist/letters', 
    split=['train', 'test'], 
    shuffle_files=True, 
    as_supervised=True, 
    with_info=True 
) 
 
def normalize_img(image, label): 
    image = tf.cast(image, tf.float32) / 255.0 
    return image, label 
 
BATCH_SIZE = 128 
 
ds_train = 
ds_train.map(normalize_img).batch(BATCH_SIZE).prefetch(tf.data.AUTOT
 UNE) 
ds_test = 
ds_test.map(normalize_img).batch(BATCH_SIZE).prefetch(tf.data.AUTOTU
 NE) 


--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Practical 8: Write a program to develop Autoencoders using MNIST Handwritten Digits. 

Code: 
import tensorflow as tf 
import numpy as np 
import matplotlib.pyplot as plt 
 
# Load MNIST dataset 
(x_train, _), (x_test, _) = tf.keras.datasets.mnist.load_data() 
 
# Normalize pixel values to [0, 1] 
x_train = x_train.astype('float32') / 255. 
x_test = x_test.astype('float32') / 255. 
 
# Reshape data for the autoencoder 
x_train = np.reshape(x_train, (len(x_train), 28, 28, 1)) 
x_test = np.reshape(x_test, (len(x_test), 28, 28, 1)) 
 
# Define the autoencoder architecture 
input_img = tf.keras.Input(shape=(28, 28, 1)) 
 
# Encoder 
x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', 
padding='same')(input_img) 
x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x) 
x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', 
padding='same')(x) 
encoded = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x) 
 
# Decoder 
x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', 
padding='same')(encoded) 
x = tf.keras.layers.UpSampling2D((2, 2))(x) 
x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', 
padding='same')(x) 
x = tf.keras.layers.UpSampling2D((2, 2))(x) 
decoded = tf.keras.layers.Conv2D(1, (3, 3), activation='sigmoid', 
padding='same')(x) 
 
# Create the autoencoder model 
autoencoder = tf.keras.Model(input_img, decoded) 
 
# Compile the model 
autoencoder.compile(optimizer='adam', loss='binary_crossentropy') 
 
# Train the autoencoder 
autoencoder.fit(x_train, x_train, 
                epochs=10, 
                batch_size=128, 
                shuffle=True, 
                validation_data=(x_test, x_test)) 
 
# Reconstruct images using the trained autoencoder 
decoded_imgs = autoencoder.predict(x_test) 
 
# Display the original and reconstructed images 
n = 10 
plt.figure(figsize=(20, 4)) 
for i in range(n): 
    # Display original 
    ax = plt.subplot(2, n, i + 1) 
    plt.imshow(x_test[i].reshape(28, 28)) 
    plt.gray() 
    ax.get_xaxis().set_visible(False) 
    ax.get_yaxis().set_visible(False) 
 
    # Display reconstruction 
    ax = plt.subplot(2, n, i + 1 + n) 
    plt.imshow(decoded_imgs[i].reshape(28, 28)) 
    plt.gray() 
    ax.get_xaxis().set_visible(False) 
    ax.get_yaxis().set_visible(False) 
plt.show() 


--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Practical 9: Demonstrate recurrent neural network that learns to perform sequence analysis for stock price.(google stock price). 

Code: 
import yfinance as yf 
import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt 
from sklearn.preprocessing import MinMaxScaler 
import tensorflow as tf 
from tensorflow.keras.models import Sequential 
from tensorflow.keras.layers import SimpleRNN, Dense 
 
# Download Google Stock Price Data 
df = yf.download('GOOG', start='2015-01-01', end='2023-12-31') 
 
df.head() 
 
# Use 'Close' Price for prediction 
data = df['Close'].values.reshape(-1, 1) 
 
# Normalize the data 
scaler = MinMaxScaler(feature_range=(0, 1)) 
data_scaled = scaler.fit_transform(data) 
 
# Create Sequences 
def create_dataset(dataset, time_step=60): 
    X, Y = [], [] 
    for i in range(len(dataset) - time_step - 1): 
        X.append(dataset[i:i+time_step, 0]) 
        Y.append(dataset[i+time_step, 0]) 
    return np.array(X), np.array(Y) 
 
time_step = 60 
X, y = create_dataset(data_scaled, time_step) 
 
# Reshape for RNN Input (samples, timesteps, features) 
X = X.reshape(X.shape[0], X.shape[1], 1) 
 
# Split into Training and Testing 
train_size = int(len(X) * 0.7) 
X_train, X_test = X[:train_size], X[train_size:] 
y_train, y_test = y[:train_size], y[train_size:] 
 
model = Sequential() 
model.add(SimpleRNN(50, return_sequences=False, 
input_shape=(time_step, 1))) 
model.add(Dense(1)) 
 
model.compile(optimizer='adam', loss='mean_squared_error') 
model.summary() 
 
history = model.fit(X_train, y_train, validation_data=(X_test, 
y_test), epochs=50, batch_size=32, verbose=1) 
 
# Predict Stock Prices 
train_predict = model.predict(X_train) 
test_predict = model.predict(X_test) 
 
# Inverse Transform to get actual values 
train_predict = scaler.inverse_transform(train_predict) 
test_predict = scaler.inverse_transform(test_predict) 
real_y_test = scaler.inverse_transform(y_test.reshape(-1, 1)) 
 
# Plotting 
plt.figure(figsize=(10,6)) 
plt.plot(df['Close'].index[train_size+time_step+1:], real_y_test, 
label="Actual Google Stock Price") 
plt.plot(df['Close'].index[train_size+time_step+1:], test_predict, 
label="Predicted Stock Price") 
plt.xlabel("Date") 
plt.ylabel("Stock Price") 
plt.legend() 
plt.title("Google Stock Price Prediction using RNN") 
plt.show() 
 

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Practical 10: Applying Generative Adversarial Networks for image generation and unsupervised tasks. 

Code: 
import numpy as np 
import tensorflow as tf 
from tensorflow.keras import layers 
import matplotlib.pyplot as plt 
 
(x_train, _), (_, _) = tf.keras.datasets.mnist.load_data() 
 
# Normalize the data between -1 and 1 for GAN 
x_train = (x_train - 127.5) / 127.5 
x_train = x_train.reshape(x_train.shape[0], 28, 28, 1) 
 
print("Training data shape:", x_train.shape) 
 
def build_generator(): 
    model = tf.keras.Sequential() 
    model.add(layers.Dense(256, input_dim=100)) 
    model.add(layers.LeakyReLU(alpha=0.2)) 
    model.add(layers.BatchNormalization(momentum=0.8)) 
    model.add(layers.Dense(512)) 
    model.add(layers.LeakyReLU(alpha=0.2)) 
    model.add(layers.BatchNormalization(momentum=0.8)) 
    model.add(layers.Dense(1024)) 
    model.add(layers.LeakyReLU(alpha=0.2)) 
    model.add(layers.BatchNormalization(momentum=0.8)) 
    model.add(layers.Dense(28 * 28 * 1, activation='tanh')) 
    model.add(layers.Reshape((28, 28, 1))) 
    return model 
 
def build_discriminator(): 
    model = tf.keras.Sequential() 
    model.add(layers.Flatten(input_shape=(28, 28, 1))) 
    model.add(layers.Dense(512)) 
    model.add(layers.LeakyReLU(alpha=0.2)) 
    model.add(layers.Dense(256)) 
    model.add(layers.LeakyReLU(alpha=0.2)) 
    model.add(layers.Dense(1, activation='sigmoid')) 
    return model 
 
# Build and Compile Discriminator 
discriminator = build_discriminator() 
discriminator.compile(loss='binary_crossentropy', optimizer='adam', 
metrics=['accuracy']) 
 
# Build Generator 
generator = build_generator() 
 
# GAN Model (Stacked) 
z = layers.Input(shape=(100,)) 
img = generator(z) 
discriminator.trainable = False 
validity = discriminator(img) 
 
# Compile GAN 
gan = tf.keras.Model(z, validity) 
gan.compile(loss='binary_crossentropy', optimizer='adam') 
 
import os 
 
epochs = 5000 
batch_size = 128 
save_interval = 1000 
 
for epoch in range(epochs): 
    # Train Discriminator 
    idx = np.random.randint(0, x_train.shape[0], batch_size) 
    real_imgs = x_train[idx] 
 
    noise = np.random.normal(0, 1, (batch_size, 100)) 
    gen_imgs = generator.predict(noise, verbose=0) 
 
    d_loss_real = discriminator.train_on_batch(real_imgs, 
np.ones((batch_size, 1))) 
    d_loss_fake = discriminator.train_on_batch(gen_imgs, 
np.zeros((batch_size, 1))) 
    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake) 
 
    # Train Generator 
    noise = np.random.normal(0, 1, (batch_size, 100)) 
    g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1))) 
 
    if epoch % save_interval == 0: 
        print(f"{epoch} [D loss: {d_loss[0]}] [G loss: {g_loss}]") 
 
def plot_generated_images(generator, examples=25, dim=(5, 5)): 
    noise = np.random.normal(0, 1, (examples, 100)) 
    generated_images = generator.predict(noise) 
 
generated_images = 0.5 * generated_images + 0.5  # Rescale to 
[0,1] 
plt.figure(figsize=(5, 5)) 
for i in range(generated_images.shape[0]): 
plt.subplot(dim[0], dim[1], i+1) 
plt.imshow(generated_images[i, :, :, 0], cmap='gray') 
plt.axis('off') 
plt.tight_layout() 
plt.show() 
plot_generated_images(generator) 