PRACTICAL 10:

Practical 10: Applying Generative Adversarial Networks for image generation and unsupervised tasks. 

Code: 
import numpy as np 
import tensorflow as tf 
from tensorflow.keras import layers 
import matplotlib.pyplot as plt 
 
(x_train, _), (_, _) = tf.keras.datasets.mnist.load_data() 
 
# Normalize the data between -1 and 1 for GAN 
x_train = (x_train - 127.5) / 127.5 
x_train = x_train.reshape(x_train.shape[0], 28, 28, 1) 
 
print("Training data shape:", x_train.shape) 
 
def build_generator(): 
    model = tf.keras.Sequential() 
    model.add(layers.Dense(256, input_dim=100)) 
    model.add(layers.LeakyReLU(alpha=0.2)) 
    model.add(layers.BatchNormalization(momentum=0.8)) 
    model.add(layers.Dense(512)) 
    model.add(layers.LeakyReLU(alpha=0.2)) 
    model.add(layers.BatchNormalization(momentum=0.8)) 
    model.add(layers.Dense(1024)) 
    model.add(layers.LeakyReLU(alpha=0.2)) 
    model.add(layers.BatchNormalization(momentum=0.8)) 
    model.add(layers.Dense(28 * 28 * 1, activation='tanh')) 
    model.add(layers.Reshape((28, 28, 1))) 
    return model 
 
def build_discriminator(): 
    model = tf.keras.Sequential() 
    model.add(layers.Flatten(input_shape=(28, 28, 1))) 
    model.add(layers.Dense(512)) 
    model.add(layers.LeakyReLU(alpha=0.2)) 
    model.add(layers.Dense(256)) 
    model.add(layers.LeakyReLU(alpha=0.2)) 
    model.add(layers.Dense(1, activation='sigmoid')) 
    return model 
 
# Build and Compile Discriminator 
discriminator = build_discriminator() 
discriminator.compile(loss='binary_crossentropy', optimizer='adam', 
metrics=['accuracy']) 
 
# Build Generator 
generator = build_generator() 
 
# GAN Model (Stacked) 
z = layers.Input(shape=(100,)) 
img = generator(z) 
discriminator.trainable = False 
validity = discriminator(img) 
 
# Compile GAN 
gan = tf.keras.Model(z, validity) 
gan.compile(loss='binary_crossentropy', optimizer='adam') 
 
import os 
 
epochs = 5000 
batch_size = 128 
save_interval = 1000 
 
for epoch in range(epochs): 
    # Train Discriminator 
    idx = np.random.randint(0, x_train.shape[0], batch_size) 
    real_imgs = x_train[idx] 
 
    noise = np.random.normal(0, 1, (batch_size, 100)) 
    gen_imgs = generator.predict(noise, verbose=0) 
 
    d_loss_real = discriminator.train_on_batch(real_imgs, 
np.ones((batch_size, 1))) 
    d_loss_fake = discriminator.train_on_batch(gen_imgs, 
np.zeros((batch_size, 1))) 
    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake) 
 
    # Train Generator 
    noise = np.random.normal(0, 1, (batch_size, 100)) 
    g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1))) 
 
    if epoch % save_interval == 0: 
        print(f"{epoch} [D loss: {d_loss[0]}] [G loss: {g_loss}]") 
 
def plot_generated_images(generator, examples=25, dim=(5, 5)): 
    noise = np.random.normal(0, 1, (examples, 100)) 
    generated_images = generator.predict(noise) 
 
generated_images = 0.5 * generated_images + 0.5  # Rescale to 
[0,1] 
plt.figure(figsize=(5, 5)) 
for i in range(generated_images.shape[0]): 
plt.subplot(dim[0], dim[1], i+1) 
plt.imshow(generated_images[i, :, :, 0], cmap='gray') 
plt.axis('off') 
plt.tight_layout() 
plt.show() 
plot_generated_images(generator) 


Output: 
Downloading data from https://storage.googleapis.com/tensorflow/tf
keras-datasets/mnist.npz 
11490434/11490434 ━━━━━━━━━━━━━━━━━━━━ 0s 0us/step 
Training data shape: (60000, 28, 28, 1) 
/usr/local/lib/python3.11/dist
packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not 
pass an `input_shape`/`input_dim` argument to a layer. When using 
Sequential models, prefer using an `Input(shape)` object as the first 
layer in the model instead. 
super().__init__(**kwargs) 
/usr/local/lib/python3.11/dist
packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: 
Argument `alpha` is deprecated. Use `negative_slope` instead. 
warnings.warn( 
/usr/local/lib/python3.11/dist
packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an 
`input_shape`/`input_dim` argument to a layer. When using Sequential 
models, prefer using an `Input(shape)` object as the first layer in the 
model instead. 
super().__init__(activity_regularizer=activity_regularizer, **kwargs) 
/usr/local/lib/python3.11/dist
packages/keras/src/backend/tensorflow/trainer.py:82: UserWarning: The 
model does not have any trainable weights. 
warnings.warn("The model does not have any trainable weights.") 
0 [D loss: 0.42612382769584656] [G loss: 0.6547456979751587] 
1000 [D loss: 4.274489879608154] [G loss: 0.0027400576509535313] 
2000 [D loss: 4.733832836151123] [G loss: 0.0013957377523183823] 
3000 [D loss: 5.022982597351074] [G loss: 0.000938069773837924] 
4000 [D loss: 5.239616394042969] [G loss: 0.0007068223785609007] 
1/1 ━━━━━━━━━━━━━━━━━━━━ 1s 573ms/step 
