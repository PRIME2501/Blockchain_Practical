PRACTICAL 6:

Practical 6: Applying the Autoencoder algorithms for encoding real-world data. 

Code: 
import tensorflow as tf 
from tensorflow.keras.models import Model 
from tensorflow.keras.layers import Input, Dense 
from sklearn.datasets import load_breast_cancer 
from sklearn.preprocessing import MinMaxScaler 
import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt 
 
# Load Breast Cancer Dataset 
data = load_breast_cancer() 
X = data.data 
print("Original Shape of Data:", X.shape) 
 
# Normalize Data 
scaler = MinMaxScaler() 
X_scaled = scaler.fit_transform(X) 
 
input_dim = X_scaled.shape[1]  # Number of features 
encoding_dim = 10  # Encoded representation size 
 
# Encoder 
input_layer = Input(shape=(input_dim,)) 
encoded = Dense(encoding_dim, activation='relu')(input_layer) 
 
# Decoder 
decoded = Dense(input_dim, activation='sigmoid')(encoded) 
 
# Autoencoder Model 
autoencoder = Model(inputs=input_layer, outputs=decoded) 
 
# Encoder Model 
encoder = Model(inputs=input_layer, outputs=encoded) 
 
autoencoder.compile(optimizer='adam', loss='mse') 
autoencoder.summary() 
 
history = autoencoder.fit(X_scaled, X_scaled, 
                          epochs=100, 
                          batch_size=32, 
                          validation_split=0.2) 
 
# Encoded Data 
encoded_data = encoder.predict(X_scaled) 
print("Encoded Data Shape:", encoded_data.shape) 
# Reconstructed Data 
decoded_data = autoencoder.predict(X_scaled) 
plt.plot(history.history['loss'], label='Training Loss') 
plt.plot(history.history['val_loss'], label='Validation Loss') 
plt.xlabel('Epoch') 
plt.ylabel('Loss') 
plt.legend() 
plt.title('Autoencoder Loss Curve') 
plt.show() 
print("Original Data Sample:\n", X_scaled[0]) 
print("Reconstructed Data Sample:\n", decoded_data[0]) 
print("Encoded Data Sample:\n", encoded_data[0]) 


Output: 
Original Shape of Data: (569, 30) 
Model: "functional_26" 
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓ 
┃ Layer (type)                    
┃ Output Shape           
┃       
Param # ┃ 
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩ 
│ input_layer_17 (InputLayer)     
│ (None, 30)             
│             
0 │ 
├─────────────────────────────────┼────────────────────────┼───────────────┤ 
│ dense_16 (Dense)                
│ (None, 10)             
│           
310 │ 
├─────────────────────────────────┼────────────────────────┼───────────────┤ 
│ dense_17 (Dense)                
│ (None, 30)             
│           
330 │ 
└─────────────────────────────────┴────────────────────────┴───────────────┘ 
Total params: 640 (2.50 KB) 
Trainable params: 640 (2.50 KB) 
Non-trainable params: 0 (0.00 B) 
Epoch 95/100 
15/15 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 0.0058 - 
val_loss: 0.0063 
Epoch 96/100 
15/15 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 0.0056 - 
val_loss: 0.0062 
Epoch 97/100 
15/15 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - loss: 0.0059 - 
val_loss: 0.0062 
Epoch 98/100 
15/15 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 0.0058 - 
val_loss: 0.0061 
Epoch 99/100 
15/15 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 0.0059 - 
val_loss: 0.0060 
Epoch 100/100 
15/15 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step - loss: 0.0056 - 
val_loss: 0.0060 
18/18 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step 
Encoded Data Shape: (569, 10) 
18/18 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step 
 Original Data Sample: 
 [0.52103744 0.0226581  0.54598853 0.36373277 0.59375282 0.7920373 
 0.70313964 0.73111332 0.68636364 0.60551811 0.35614702 0.12046941 
 0.3690336  0.27381126 0.15929565 0.35139844 0.13568182 0.30062512 
 0.31164518 0.18304244 0.62077552 0.14152452 0.66831017 0.45069799 
 0.60113584 0.61929156 0.56861022 0.91202749 0.59846245 0.41886396] 
Reconstructed Data Sample: 
 [0.5242122  0.27852187 0.571125   0.43847564 0.6478379  0.7026527 
 0.7056049  0.715928   0.65106314 0.5575754  0.30277127 0.19349438 
 0.18168852 0.17325903 0.23431863 0.48995662 0.23359178 0.4444599 
 0.2571366  0.29156366 0.563843   0.3945287  0.59733844 0.46849254 
 0.6303863  0.6792389  0.72714144 0.875585   0.48770216 0.5371881 ] 
Encoded Data Sample: 
 [0.         0.         3.650733   0.45598152 0.         0.7523843 
 0.         0.86943805 0.9158503  0.        ] 
